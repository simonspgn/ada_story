---
layout: post
title: "A Data Analysis on Political Popularity and Media Coverage"
subtitle: "EPFL Applied Data Analysis Final Project"
date: 2021-12-10 23:45:13 -0400
background: '/img/posts/elections4.jpg'
---

Welcome to Team SARS' final ADAmazing project. In this study, we analyze the intricate relationship between voting intentions and media coverage in the past three US presidential elections. We find that...

# Background
It is often argued that former US president Donald Trump’s upset victory in the 2016 elections strongly relates to his ability to monopolize the attention of mass media outlets [1]. Although the former president’s polemical nature granted him a front row seat at almost every (inter)national newspaper in the months and years preceding the election [2], it can be a difficult task to assess the impact that such incessant media coverage had on the final outcome of the presidential elections. This is precisely the question that we aim to answer in the following study. We use the quotebank corpus to analyse possible correlations and causations between the media coverage of three different major outlets with notoriously distinct political inclinations: CNN (democrat), BBC (centrist) and Fox News (republican) as well as voting intentions in the months preceding each elections.

One of the first questions that we were faced with related to finding accurate means of quantifying media coverage and voting intentions. As such, we define the following:

* **Media Coverage**: defined in our study by the number of occurrences of keywords (such as a presidential candidate's name or his/her running mate) during a given presidential election in a given newspaper (BBC, NY Times or Fox News).

* **Voting Intentions**: defined in our study by poll numbers for each presidential candidate in the months prior to a given election.

# A visual tour of media coverage & voting intentions
In this section, we present a visual description of the data parsing and manipulations performed in order to begin our study.

## Media Coverage
Retrieving media coverage was done solely through the Quotebank dataset [3]. We started by retrieving all the quotations for the years in which a presidential election occurred (2012, 2016 and 2020) and which originated from one of the following three newspapers: NYTimes, Fox News, BBC (done by checking the origin of each link in the dataset's url column).

For each of the parsed quotation, we kept only those that included at least one occurrence of the presidential candidates names (or their running mates) within the quotation, the speaker or the url attributes of the dataset. Finally, we built timelines of the retrieved occurrence for each of the three aforementioned newspapers in the months preceding the presidential elections (January - November) in 2012, 2016 and 2020.

We start by presenting a visualization regarding the number of mentions per month for each newspaper that we have retrieved when parsing the data. This part does not focus on the candidates but rather on the overall occurrences of any keyword of interest per newspaper.

<iframe src="/ada_story/img/posts/newspapers_over_years.html" height="500px" width="100%" frameBorder="0"></iframe>

This initial visualization gives us several important insights for our study:

* **First**: the data for the 2012 elections that we retrieved is fairly well represented with generally more mentions in the NY Times, somewhat closely followed by Fox News and finally by BBC which (as expected) has the least mentions- we believe that BBC should have the least mentions because it isn't a natively American newspaper and since it is generally regarded as a more "centrist" leaning - in contrast to NY Times that is typically democrat leaning and Fox News that is republican leaning.

* **Second**: we are missing a considerable amount of data for the months of January-June, October 2016 and May-November 2020. This could potentially mean that there are only very few mentions in the newspapers we target (which seems highly unlikely considering the general fuss in the months leading to the elections and by comparing it with the 2012 data retrieved). A more plausible explanation refers to the quality of the Quotebank dataset, simply missing relevant quotations in these months. Concerning 2020, we noted that the Quotebank data retrieval ended in April 2020 [3], which confirms our observation. Concerning the 2016, we note in the visualization below that the missing data is simply caused by the poor nature of the dataset.

<iframe src="/ada_story/img/posts/bubble_2016_statistics.html" height="500px" width="100%" frameBorder="0"></iframe>

We illustrate in the bubble chart the total number of quotations present in the Quotebank dataset for a given newspaper in a given month (indicated by the larger transparent bubbles) versus the number of quotations retrieved that include any of our keywords of insterest (presidential candidate name or running mate name) for a given newspaper in a given month. We distinctly note that the Quotebank dataset exhibits very few total quotations for any of the newspapers in the months of January to June (inclusive) and October. To this observation we add the fact that the authors of Quotebank mention that.... [ANTOINE WHERE DID YOU FIND IT IN THE PAPER - ADD HERE]


We then proceed in our analysis by visualizing the distribution of mentions per candidate retrieved for the three years of interest (independently of the newspaper in which the mention occurred). We see in the visualization below a very interesting trend in which the republican candidates are generally more represented/mentioned in the three elections (without distinguishing between newspapers). Although the difference between Obama and Romney remains somewhat limited, the few months we can observe in the 2016 and 2020 elections show a complete disproportion in coverage between Trump and the democratic candidate.

<iframe src="/ada_story/img/posts/candidates_over_year.html" height="500px" width="100%" frameBorder="0"></iframe>

It is also interesting to note the overall difference between the republican and the democratic candidate over the span of the three elections. In 2012, we found 2695 more quotation referring to the republican candidate (Mitt Romney) than the democratic candidate (Barack Obama). This number is then increased almost two-fold (5460 quotations in 2016 and 4245 quotations in 2020) for the 2016 and 2020 elections. Even considering all the missing data in our set for 2016 and 2020, this is a notable difference that may be even more drastic with a full dataset. Such an observation may raise the question on whether the arrival of Donald Trump as a presidential candidate motivated an increase in media coverage difference between republican and democratic candidate.

We can further delve into the difference in coverage between republican and democratic candidate by visualizing the distribution of quotations referring to each candidate on a newspaper basis.

<iframe src="/ada_story/img/posts/candidates_over_year_newspaper.html" height="500px" width="100%" frameBorder="0"></iframe>

We see that tendency previously mentioned to predominantly cover a republican candidate is not necessarily true for 2012: although NYTimes (a notoriously Democrat newspaper) shows stronger media coverage for Mitt Romney, we note that Fox News (a notoriously Republican newspaper) shows slightly higher media coverage for Barack Obama. This is a very interesting behavior that is not apparent when looking only at overall mentions across all newspapers.

The only month in 2012 in which Barack Obama was given more NYTimes media coverage was in May. We also see that the highest democratic candidate coverage in the Fox News newspaper was during that same month. Can these peaks be attributed to a particular event? After a quick online search we found that in May 2012, Barack Obama became the first sitting US president to announce support for same-sex marriage [4]. Could these events be related? A more fined grain analysis may reveal correlation between these two events.

Nevertheless, in 2016 and 2020, the republican candidate always gets more media coverage than the democratic one.

## Voting Intentions

After scraping the polls the voting intentions taken from different surveys accross the United States from January 2012/2016/2020 to November 2012/2016/2020, we’ve plotted the line plots per candidate and per year

<iframe src="/ada_story/img/posts/voting_intentions_allyears.html" height="500px" width="100%" frameBorder="0"></iframe>

# Getting our hands dirty

We perform the correlation study here. We first start by a baseline model in which we compare media coverage and voting intentions on a per week? basis. We use regression analysis/statistical tests.... FILL IN HERE
We then further the analysis by including sentiment analysis as a covariate.... FILL IN HERE

In this section, we’ll study the correlation between the media coverage quantified and the voting intentions. First we’ve implemented a very simple baseline model… and then we’ve enhanced it by using unsupervised sentiment analysis…

There are several timeframe levels (such as year, month, week and day) in which we can aggregate the results for each combination of election year, candidate, and newspaper. If we aggregate by year we will have 1 data point only, by month 12 data points at most and 3 in the worst case (because some months will be discarded due to the small amount of occurrences). Due to the lack of data in 2016 and 2020 we found that the it may be a better  compromise between number of occurrences per period of time and the number of data points is achieved when we aggregate by week. The aggregation is performed as follows: take the mean of intention votes over the week (by choosing the end date of the poll to specify the week), and the occurrences will be rescaled by taking the occurrences of quotes for a specific combination over the week and dividing it by the total number of quotations in the specific week and newspaper.

## Baseline Correlation Study Between Media Coverage & Voting intentions

NOTE: Since we use a p-value to confirm/reject our correlation study, we may have to expressively describe what the null hypothesis is.

We started our correlation study by applying Spearman's rank correlation analysis between the media coverage rescaled (EXPLAIN HERE) and the voting intentions for each candidate in a given newspaper. We choose this metric as we noted that the media coverage variable is not normally distributed thus.... (EXPAND HERE - What does it mean? What's the implication of not having a normally distributed media coverage? Why could we not use other models?). We view our model as having intention vote as the dependent variable and media coverage as the predictor. The correlation results are illustrated in the chart below. Note that correlations in which the p-value is smaller than 0.05 (cases in which our hypothesis is statistically significant) are marked with dotted patterns to distinguish them from p-values that cannot be rejected (>0.05). We also note that the correlation study for 2016 and 2020 is performed only for the months in which we deemed to have enough media coverage data (2016: July, August, September; 2020: January, February, March, April). We did not include any specific statistical study or threshold to pick the months with "enough" data. Rather, we pick these months based on the visualizations presented in the previous section.

<iframe src="/ada_story/img/posts/baseline_correlation.html" height="500px" width="100%" frameBorder="0"></iframe>

We present in the illustration above three categories: statistical tests in which we have clear significance where p<0.05 (marked in dark blue), statistical tests in which we observed "weak evidence" where p is between 0.05 and 0.1 (marked in light blue) and finally statistical tests that do not present any significance, where p>0.1 (marked in red).

We note a few interesting observations:

  * **Barack Obama**: We note that Fox News, BBC and NY Times exhibit a negative correlation between the media coverage and voting intentions, but these result are not statistically significant thus cannot be considered for our study (the p-values retrieved for the three newspapers are all higher than 0.1).
  * **Mitt Romney**: the three newspapers exhibit positive correlations between media coverage and voting intentions. We note that for the NY Times and Fox News newspaper, the correlation study is supported by a small p-value (thus the null hypothesis previously described can be rejected), which in turn can be a strong indication that there may exist a positive correlation between media coverage in voting intention. A similar argument can be drawn for the BBC newspaper, but this time we only note a weak indication of correlation (as the p-value is between 0.05 and 0.1).  
  Thus, concerning the overall 2012 elections, we may observe a correlation between media coverage and voting intentions for candidate Romney. However, nothing can be concluded for candidate Obama.
  * **Hillary Clinton**: we note overall positive correlations for the three newspapers. However, only the correlation exhibited by the BBC newspaper can be considered since it is the only test in which we have statistical significance (the p-value is smaller than 0.05).
  * **Donald Trump 2016**: we observe positive correlations in all three newspapers. Nonethless, in a similar manner as Donald Trump's opponent candidate Hillary Clinton, we only witness statistical significance for the BBC newspaper.  
  * **Joe Biden**: although we note only positive correlations between media coverage and voting intentions in the three newspapers, no conclusion can be draw as all correlations exhibit a p-value higher than the 0.1 threshold. The null hypothesis cannot be rejected.
  * **Donald Trump 2020**: The study conducted for candidate Donald Trump in 2020 exhibits a similar behavior as the one done for his opponent Joe Biden. We note positive correlations for BBC and Fox News and a negative correlation for NY Times. However, since the p-values are all higher than our threshold, we cannot reject the null hypothesis.

This first baseline correlation study illustrates a few interesting behaviors. First, it is intriguing to note that the BBC newspaper in 2016 seems to exhibit an overall positive correlation between media coverage and voting intentions for both candidates (Hillary Clinton and Donald Trump 2016) - both with a fairly similar spearman coefficient. We believe that such an observation highlights the "centrist" nature of the newspaper, noting here that in a newspaper such as BBC, any mention of a candidate may incur an increase in voting intentions. However, it is also important to highlight that in our original dataset, we have significantly less quotations originating from the BBC newspaper than from NY Times or Fox News. Considering the fact that we barely note any statistical significance for the correlation study of the latter two newspapers, it is possible that this discrepancy in overall quotations may have an effect on the study. However, it is difficult to determine the significance of such a possible impact (if any). Ideally, the study would be conducted with an equal and sufficient number of quotations originating from each newspaper. Alternatively, another inquiry that may arise from these observations, may question the reason behind the fact that we do not observe statistical significance for all the candidates in the BBC newspaper.

A second interesting observation relates to the results retrieved in the three newspapers concerning candidate Mitt Romney. As previously mentioned, we note that any coverage could potentially causes a raise in voting intentions. Such a relationship may make sense in the case of a republican newspaper (possibly a centrist one as well), since Mitt Romney is himself a Republican candidate. However, how could we explain the statistically significant correlation observed in NY Times for Mitt Romney? Would it make sense for a notoriously Democratic Newspaper to have a positive effect on the voting intentions of a Republican candidate? It is difficult to answer such a question without taking into account the context in which the quotations were made, specifically regarding the overall sentiment of the quotations.

As such, we decide to expand our correlation study by including a sentiment analysis performed on each of the retrieved quotations. We note that drawing any conclusions from the results retrieved in the first baseline correlation study may be an arduous task since there may be external covariates that we do not take into account influencing the results observed. One such covariate may be the sentiment of the quotations. If we can, for instance, determine that most of the quotations retrieved for the NY Times newspaper about Mitt Romney have a positive connotation, we may explain why the observed correlation between media coverage and voting intentions is positive (although it would give us no insight as to why a Democratic-leaning newspaper would include positive connotations about a Republican candidate).

Additionally, simply studying the baseline correlation between candidates Hillary Clinton and Donald Trump in 2016 gives us little information regarding the nature of the potential correlation. Indeed, we would interpret our results in complete different manners if we note that the positive correlation between media coverage and voting intention is caused principally by quotations deemed negative versus if it is caused mostly by quotations deemed positive.

Consequently, we present in the next section the sentiment analysis conducted on our datasets and the results achieved, with some possible interpretations of the observations witnessed. 


## Adding Sentiment Analysis

Until now we've considered all candidate's name mentions with the same weight but we argue that two quotes with a
candidate's name mention can have very different effect based on positive or negative sentiment towards the candidate.
Therefore, we want expand our analysis with an additional dimension : polarity. We hope this can guide our previous
results.

Given our finite time, we have to assume properties :

* [Sentiment correctness] We assume the aggregated sentiment of the quote reflect the general sentiment toward the
  candidate i.e there is a negligible number of hypocrite, humorous or 2nd degree quotes.
* [No external bias] QuoteBank is a non-biased sample of media coverage. This would be the case if QuoteBank was a
  complete set of all *words* used in the newspaper for the last 15 years, which is not feasible. QuoteBank is biased
  but we suppose for the sack of analysis its bias does not interfere.

Since we don't own a labeled dataset, we opted for unsupervised sentiment analysis which unfortunately makes it
difficult to assess the accuracy. On the other hand, manually labelling quotes (>10'000) would have been impossible in
the given timeframe.

### Sentiment Classification

Our model will take text as input and output the sentiment associated to it.

1. text:- Actual quote
2. sentiment:- Floating value between -1 (negative) and +1 (positive).

### Preprocessing

We applied a simple preprocessing pipeline to prepare quotes for sentiment analysis. The first step comes with removing
the noises in the data; noise is referred to as something which not related to textual human language, and those come
with various nature like special characters, use of parentheses, use of square brackets, white spaces, URL’s and
punctuations.

Then, text normalisation starts with tokenizing the text, which our longer corpus is now to be split into words, which
the tokenizer class from NLTK can do. Post that, we need to lower case each word of our corpus, converting numbers to
the words, contraction replacement and last stemming and lemmatization.

-- Pipeline --

* Remove punctuation
* Make text lowercase
* Tokenization
* Remove stopwords
* Stemming
* Lemmatization

Example:
**Original quote** : *Trump's a tough guy to vote for*.
**Preprocessed quote** : *trump tough guy vote*.

### Sentiment Analysis Implementation

First let’s have a look the implementation of the unsupervised sentiment analysis on each quote that mentioned one of
our candidates in 2012/2016/2020.

We tried three
different [unsupervised models](https://github.com/epfl-ada/ada-2021-project-sars/tree/main/sentiment_analysis#:~:text=unsupervised_sentiment_analysis.ipynb) :

* [VADER](https://github.com/cjhutto/vaderSentiment) (Valence Aware Dictionary and sEntiment Reasoner) is a lexicon and rule-based sentiment analysis tool that is specifically attuned to sentiments expressed in social media. It predicts the sentiment of the quote as a float, positive values are positive valence, negative value are negative valence.
* [Pre-trained BERT](https://huggingface.co/nlptown/bert-base-multilingual-uncased-sentiment) with >700 embeddings (bert-base-multilingual-uncased-sentiment). It predicts the sentiment of the review as a number of stars (between 1 and 5) and ensures 65% accuracy on their testing dataset.
* [TextBlob](https://textblob.readthedocs.io/en/dev/quickstart.html#sentiment-analysis). It aims to provide access to common text-processing operations through a familiar interface. The polarity score is a float within the range [-1.0, 1.0]. The subjectivity is a float within the range [0.0, 1.0] where 0.0 is very objective and 1.0 is very subjective.

#### Model selection

Model selection was done using visualisation tools such as barplots, word-clouds (
see [notebook](https://github.com/epfl-ada/ada-2021-project-sars/tree/main/sentiment_analysis#:~:text=exploratory_data_analysis.ipynb))
. All further plots were done with the best model (TextBlob) which provided the most interesting properties :

* It provides an additional metric by default : subjectivity. A floating value from 0 to 1 which represents the subjectivity of the quote.
* The 'default' return value is a polarity score of 0 (neutral) which is not trivial and implemented differently by the other models. BERT outputs a majority of 1-labeled quotes (very negative) by default which would introduce inaccuracies in our dataset.
* As we aggregate the data by month/by weeks for the correlation analysis, it was decided more accurate to use numerical data instead of categorical as polarity score.

Processing was simply done by iterating over the dataset and applying the model to each quote.

Example:
**Original quote** : *Trump's a tough guy to vote for*.
**Preprocessed quote** : *trump tough guy vote*.
**Polarity/subjectivity score** : [-0.389, 0.834].

#### Model validation

![image](/ada_story/img/posts/wordcloud_2016.png)

![image 2](/ada_story/img/posts/wordcloud_2020.png)

![image 3](/ada_story/img/posts/wordcloud_2012.png)

Word-clouds sort the most present words by size. We partition the datasets for each year by positive/negative quotes and plot their respective word-clouds. We can intuitively assess the precision of our model by looking at them. We classify quotes with strong negative polarity valence (< -0.8) as negative and strong positive polarity valence (>0.8) as positive quotes. Thus filtering all quotes with low-confidence of low polarity. We observe a majority of ['great', 'good', 'win'] for positive quotes and ['worst', 'hate'] for negative quotes.


[ADD FREQUENCY DISTRIBUTION OF POLARITY AND SUBJECTIVITY SCORES OVER 2012, 2016, 2020]

<iframe src="/ada_story/img/posts/sentiment.html" height="500px" width="100%" frameBorder="0"></iframe>

We observe that a majority of quotes are analyzed as neutral (with respect to all models). This could be explained by a bias of the dataset as well as our sentiment analysis bias. We figured that the choice of stopwords to be removed from the strings is crucial to attributing positive or negative values.

#### Evolution of polarity & subjectivity score over months prior to election

We decide to compare vote intentions with aggregated polarity of quotes on a candidate/monthly basis. Let's plot the polarity and subjectivity distributions over time aggregated by month and candidate.

<iframe src="/ada_story/img/posts/polarity.html" height="500px" width="100%" frameBorder="0"></iframe>

<iframe src="/ada_story/img/posts/subjectivity.html" height="500px" width="100%" frameBorder="0"></iframe>

Unfortunately, the change in polarity (y-axis) from a month to another is very small in most cases which makes it difficult to conclude a meaningful statistical change. However we observe a very different timeline for 2016 election due to missing data and quotes from 2020 don't extending past April. It is difficult to observe anything visually in this case. We have to repeat Spearman correlation test as above but with a twist : we partition the previous datasets (for each candidate) based on polarity (>0 positive and <0 negative) and apply correlation test on each individual partition.

Why not take into account quotes with neutral polarity ?

* A neutral score could be the result of an error of the model or over-preprocessing of the quote.
* Correlation score between neutral quotes and vote intentions is hard to interpret.



### Correlation Results:
<iframe src="/ada_story/img/posts/sentiment_correlation.html" height="600px" width="110%" frameBorder="0"></iframe>

# So, did we find ADAstonishing results?

Conclude the study here

# References
[1] _https://www.politico.com/magazine/story/2016/11/2016-election-trump-media-takeover-coverage-214419/_  
[2] _https://www.bbc.co.uk/news/36429660.amp_  
[3] _https://dl.acm.org/doi/10.1145/3437963.3441760_  
[4] _https://obamawhitehouse.archives.gov/blog/2012/05/10/obama-supports-same-sex-marriage_  
